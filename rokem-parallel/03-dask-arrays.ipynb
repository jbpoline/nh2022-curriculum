{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc9d3459-7f54-40ce-ba74-55ab25c2d06a",
   "metadata": {},
   "source": [
    "# Dask arrays\n",
    "\n",
    "Let's talk about the numpy-like interface.\n",
    "\n",
    "Let's say that we'd like to compute the tsnr over several runs of fMRI data, for example, using the open-fMRI dataset ds000114. We've already downloaded a session's worth of data to the hub in this location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e9e05d-e499-4aed-815f-42842c070f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "fnames = glob('/home/jovyan/shared/ds000114/sub-01/ses-test/func/*.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de12c8a4-6644-477d-a186-85a5aa9b5723",
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58278a5-651a-44c6-bb49-e539548cc422",
   "metadata": {},
   "source": [
    "This is a list with 5 different file-names, for the different runs during this session. One way to calculate the tsnr across is to loop over the files, read in the data for each one of them, concatenate the data and then compute the tsnr from the concatenated series. Note that we are using the illustrious nibabel library to read the data from disk. Nibabel uses \"lazy loading\". That means that data are not read from file when the nibabel `load` function is called on a file-name. Instead, Nibabel waits until we ask for the data, using the `get_fdata` method of The `Nifti1Image` class to read the data from file. Here, we do that immediately, but we'll use this fact a little later to our advantage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04accccd-71f3-417c-a87c-505a2306afda",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "data = []\n",
    "for fname in fnames:\n",
    "    data.append(nib.load(fname).get_fdata())\n",
    "\n",
    "data = np.concatenate(data, -1)\n",
    "tsnr = data.mean(-1) / data.std(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a93c08-68da-48a7-bfbf-df1e315f19e0",
   "metadata": {},
   "source": [
    "When we do that, most of the time is spent on reading the data from file.\n",
    "As you can probably reason yourself, the individual items in the data\n",
    "list have no dependency on each other, so they could be calculated in\n",
    "parallel.\n",
    "\n",
    "How do we approach the problem in this case? Because of nibabel's lazy-loading, we can instruct it to wait with the call to `get_fdata`. We create a delayed function that we call `delayed_get_fdata`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72740662-8679-4b84-a826-346a71addc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import delayed\n",
    "delayed_get_fdata = delayed(nib.Nifti1Image.get_fdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cfb3c4-d7e5-4dd3-a3da-190cc99ed142",
   "metadata": {},
   "source": [
    "Then, we use this function to create a list of items and delay each one of the computations on this list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46071d4-439e-4f1d-8b10-002292590769",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for fname in fnames:\n",
    "    data.append(delayed_get_fdata(nib.load(fname)))\n",
    "\n",
    "data = delayed(np.concatenate)(data, -1)\n",
    "tsnr = delayed(data.mean)(-1) / delayed(data.std)(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1578e685-8a64-4832-8773-df2b93f9e89f",
   "metadata": {},
   "source": [
    "We can see what the graph is for this computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bb6e68-4236-4d79-81a0-fa1581e32446",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsnr.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdf8344-ba7c-4d34-b406-a3e409d98ab6",
   "metadata": {},
   "source": [
    "Indeed computing tsnr this way can give you an approximately 2-fold speedup. This is because Dask allows the Python process to read several of the files in parallel, and that is the performance bottle-neck here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db5816b-32ae-4163-a23f-908d0f868bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result = tsnr.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d18269d-dfa0-41d2-a14c-ad37d75bc253",
   "metadata": {},
   "source": [
    "### Dask arrays\n",
    "\n",
    "This is already quite useful, but wouldn't you rather just tell dask that you are going to create some data and to treat it all as delayed until you are ready to compute the tsnr?\n",
    "\n",
    "This idea is implemented in the dask array interface. The idea here is that you create something that provides all of the interfaces of a numpy array, but all the computations are treated as delayed.\n",
    "\n",
    "This is what it would look like for the tsnr example. Instead of appending delayed calls to `get_fdata` into the array, we create a series of dask arrays, with `delayed_get_fdata`. We do need to know both the shape and data type of the arrays that will eventually be read, but"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6534a4-a5ea-4004-b538-213af1d0074b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "\n",
    "delayed_arrays = []\n",
    "for fname in fnames:\n",
    "    img = nib.load(fname)\n",
    "    delayed_arrays.append(da.from_delayed(delayed_get_fdata(img),\n",
    "                          img.shape,\n",
    "                          img.get_data_dtype()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54123951-53f9-4d6d-b881-7737f395ae62",
   "metadata": {},
   "outputs": [],
   "source": [
    "delayed_arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c67854-a17b-42a0-9e9c-fffb8c489ac1",
   "metadata": {},
   "source": [
    "These are notional arrays, that have not been materialized yet. The data has not been read from memory yet, although dask already knows where it would put them when they should be read.\n",
    "\n",
    "But they have a lot of properties that real arrays have, albeit with the dask array interface, instead of the one from numpy. For example, we can use the `dask.array.concatenate` function with them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2825b931-ade0-40cd-9c78-95d4c92a3838",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = da.concatenate(delayed_arrays, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249454c4-7647-41b2-9c42-195945aa081b",
   "metadata": {},
   "source": [
    "This array has some attributes that look just like numpy array attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad98e2b2-3e5e-4163-b540-20755de60feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323648bc-abc2-488c-9d7d-4e5e335b1b7a",
   "metadata": {},
   "source": [
    "On the other hand, when we ask to see it, we get a lot of information, but none of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78641274-c89a-4623-9711-57b545463dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc569f7b-6250-400d-a06a-119adf358b53",
   "metadata": {},
   "source": [
    "Nevertheless, we can then use methods of the `dask.array` object to complete the computation. The computation looks just like the computation we did with the numpy array!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531f9997-8427-4252-a8ce-36c147a30837",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsnr = arr.mean(-1) / arr.std(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51691ddd-b003-4b03-b534-e6f6f9b5cc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f990afa-af03-4e64-a533-4065d9094d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsnr.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bae7c24-4845-4b0d-97a6-dff5b1fd2b45",
   "metadata": {},
   "source": [
    "This looks really complicated, but notice that because dask has even more insight into what we are trying to do, it can delay some things until\n",
    "after aggregation. For example, the square root computation of the standard deviation can be done once at the end, instead of on each array separately.\n",
    "\n",
    "And this leads to an *additional* approximately 2-fold speedup.\n",
    "\n",
    "One of the main things to notice about the dask array is that because the data is not read into memory it can represent very large datasets, and schedule operations over these large datasets in a manner that makes the code seem as though all the data is in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c77fc16-5e38-4f21-8c3a-3e33a2357653",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result = tsnr.compute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
